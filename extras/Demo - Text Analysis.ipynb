{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================\n",
    "Working With Text Data\n",
    "======================\n",
    "\n",
    "The goal of this guide is to explore some of the main ``scikit-learn``\n",
    "tools on a single practical task: analyzing a collection of text\n",
    "documents (newsgroups posts) on twenty different topics.\n",
    "\n",
    "In this section we will see how to:\n",
    "\n",
    "  - load the file contents and the categories\n",
    "\n",
    "  - extract feature vectors suitable for machine learning\n",
    "\n",
    "  - train a linear model to perform categorization\n",
    "\n",
    "  - use a grid search strategy to find a good configuration of both\n",
    "    the feature extraction components and the classifier\n",
    "\n",
    "\n",
    "Tutorial setup\n",
    "--------------\n",
    "\n",
    "To get started with this tutorial, you must first install\n",
    "*scikit-learn* and all of its required dependencies.\n",
    "\n",
    "Please refer to the :ref:`installation instructions <installation-instructions>`\n",
    "page for more information and for system-specific instructions.\n",
    "\n",
    "The source of this tutorial can be found within your scikit-learn folder::\n",
    "\n",
    "    scikit-learn/doc/tutorial/text_analytics/\n",
    "\n",
    "The source can also be found `on Github\n",
    "<https://github.com/scikit-learn/scikit-learn/tree/main/doc/tutorial/text_analytics>`_.\n",
    "\n",
    "The tutorial folder should contain the following sub-folders:\n",
    "\n",
    "  * ``*.rst files`` - the source of the tutorial document written with sphinx\n",
    "\n",
    "  * ``data`` - folder to put the datasets used during the tutorial\n",
    "\n",
    "  * ``skeletons`` - sample incomplete scripts for the exercises\n",
    "\n",
    "  * ``solutions`` - solutions of the exercises\n",
    "\n",
    "\n",
    "You can already copy the skeletons into a new folder somewhere\n",
    "on your hard-drive named ``sklearn_tut_workspace`` where you\n",
    "will edit your own files for the exercises while keeping\n",
    "the original skeletons intact:\n",
    "\n",
    ".. prompt:: bash $\n",
    "\n",
    "  cp -r skeletons work_directory/sklearn_tut_workspace\n",
    "\n",
    "\n",
    "Machine learning algorithms need data. Go to each ``$TUTORIAL_HOME/data``\n",
    "sub-folder and run the ``fetch_data.py`` script from there (after\n",
    "having read them first).\n",
    "\n",
    "For instance:\n",
    "\n",
    ".. prompt:: bash $\n",
    "\n",
    "  cd $TUTORIAL_HOME/data/languages\n",
    "  less fetch_data.py\n",
    "  python fetch_data.py\n",
    "\n",
    "\n",
    "Loading the 20 newsgroups dataset\n",
    "---------------------------------\n",
    "\n",
    "The dataset is called \"Twenty Newsgroups\". Here is the official\n",
    "description, quoted from the `website\n",
    "<http://people.csail.mit.edu/jrennie/20Newsgroups/>`_:\n",
    "\n",
    "  The 20 Newsgroups data set is a collection of approximately 20,000\n",
    "  newsgroup documents, partitioned (nearly) evenly across 20 different\n",
    "  newsgroups. To the best of our knowledge, it was originally collected\n",
    "  by Ken Lang, probably for his paper \"Newsweeder: Learning to filter\n",
    "  netnews,\" though he does not explicitly mention this collection.\n",
    "  The 20 newsgroups collection has become a popular data set for\n",
    "  experiments in text applications of machine learning techniques,\n",
    "  such as text classification and text clustering.\n",
    "\n",
    "In the following we will use the built-in dataset loader for 20 newsgroups\n",
    "from scikit-learn. Alternatively, it is possible to download the dataset\n",
    "manually from the website and use the :func:`sklearn.datasets.load_files`\n",
    "function by pointing it to the ``20news-bydate-train`` sub-folder of the\n",
    "uncompressed archive folder.\n",
    "\n",
    "In order to get faster execution times for this first example we will\n",
    "work on a partial dataset with only 4 categories out of the 20 available\n",
    "in the dataset::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
